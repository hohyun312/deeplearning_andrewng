{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "from utils_6 import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) How does gradient checking work?\n",
    "\n",
    "순전파는 비교적 구현이 쉽기 때문에, 역전파에 비해 올바르게 구현했을 가능성이 높다. 따라서 순전파만을 이용해 역전파를 근사할 수 있다면 역전파를 제대로 구현했는지 좀 더 확신할 수 있을 것이다. 다음 공식을 수치적으로 계산하여 순전파를 이용해 기울기를 근사적으로 구할 수 있다.\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 1-dimensional gradient checking\n",
    "\n",
    "우선 쉬운 예제로서 $J(\\theta) = \\theta x$에 대해 gradient checking을 해볼 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, theta):\n",
    "    J = theta * x\n",
    "    return J\n",
    "\n",
    "def backward_propagation(x, theta):\n",
    "    dtheta = x\n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에 있는 (1) 공식을 이용해 $\\theta$에 대한 미분값을 근사적으로 계산하고, 이를 역전파를 통해 구한 값과 비교해야 한다. 두 값의 차이는 다음 공식을 이용해 계산한다.\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$\n",
    "- 만약 차이가 작다면 (가령 $10^{-7}$보다 작다면), 역전파를 올바르게 구현했다고 자신해도 좋다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(x, theta, epsilon = 1e-7):\n",
    "    theta_plus = theta + epsilon\n",
    "    theta_minus = theta - epsilon\n",
    "    J_plus = forward_propagation(x, theta_plus)\n",
    "    J_minus = forward_propagation(x, theta_minus)\n",
    "    \n",
    "    gradapprox = (J_plus - J_minus) / (2*epsilon)\n",
    "    grad = backward_propagation(x, theta)\n",
    "    \n",
    "    difference = np.linalg.norm(grad - gradapprox) / (np.linalg.norm(grad) + np.linalg.norm(gradapprox))\n",
    "    \n",
    "    if difference < 1e-7:\n",
    "        print (\"The gradient is correct!\")\n",
    "    else:\n",
    "        print (\"The gradient is wrong!\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is correct!\n",
      "difference = 2.919335883291695e-10\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(x, theta)\n",
    "print(\"difference = \" + str(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) N-dimensional gradient checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이제 좀 더 일반화된 경우를 연습하기 위해 3층 신경망에 대한 gradient checking을 구현해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cost = -np.sum(Y * np.log(A3) + (1 - Y) * np.log(1-A3)) / m\n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache\n",
    "\n",
    "def backward_propagation_n(X, Y, cache):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = np.dot(dZ3, A2.T) / m\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True) / m\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = dA2 * (Z2>0)\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = dA1 * (Z1>0)\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "        \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전과 마찬가지 방식으로 그레이언트 검사를 한다. 단 이번에는 검사해야 할 파라미터의 개수가 많다. 각각의 파라미터 모두에 대해 위에서 했던 방식을 반복하기 위해서, 모든 파라미터를 하나의 백터로 만들어서 계산하는 방식을 사용해보자. 이를 위해 사전에 정의된 `dictionary_to_vector` 등의 함수를 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
    "    \n",
    "    # 모든 파라미터(\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\")를 (-1, 1) 크기를 가진 하나의 벡터로 만든다.\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    \n",
    "    # 모든 그레디언트(backward_propagation_n의 반환값)를 (-1, 1) 크기를 가진 하나의 벡터로 만든다.\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    \n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # gradapprox 계산하기\n",
    "    for i in range(num_parameters):\n",
    "        theta_plus = np.copy(parameters_values)\n",
    "        theta_plus[i, 0] += epsilon\n",
    "        J_plus, _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_plus))\n",
    "\n",
    "        theta_minus = np.copy(parameters_values)\n",
    "        theta_minus[i, 0] -= epsilon\n",
    "        J_minus, _ = forward_propagation_n(X, Y, vector_to_dictionary(theta_minus))\n",
    "        gradapprox[i] = (J_plus - J_minus) / (2*epsilon)\n",
    "        \n",
    "    difference = np.linalg.norm(grad - gradapprox) / (np.linalg.norm(grad)+np.linalg.norm(gradapprox))\n",
    "\n",
    "    if difference > 2e-7:\n",
    "        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.3607563281878912e-09\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 잘 작동하는지 확인하기 위해 임의의 값들을 넣어서 확인.\n",
    "def gradient_check_n_test_case(): \n",
    "    x = np.random.randn(4,3)\n",
    "    y = np.array([1, 1, 0])\n",
    "    W1 = np.random.randn(5,4) \n",
    "    b1 = np.random.randn(5,1) \n",
    "    W2 = np.random.randn(3,5) \n",
    "    b2 = np.random.randn(3,1) \n",
    "    W3 = np.random.randn(1,3) \n",
    "    b3 = np.random.randn(1,1) \n",
    "    parameters = {\"W1\": W1,\"b1\": b1,\"W2\": W2,\"b2\": b2,\"W3\": W3,\"b3\": b3}\n",
    "    return x, y, parameters\n",
    "\n",
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그레디언트를 수치적으로 계산하는 것은 느리기 때문에 코드가 제대로 짜여졌는지 확인하는 용도로만 사용한다.\n",
    "- 드롭아웃을 사용할 경우, 우선 드롭아웃이 없는 채로 그레디언트 검증을 사용하고 드롭아웃을 추가한다.\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "n6NBD",
   "launcher_item_id": "yfOsE"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
